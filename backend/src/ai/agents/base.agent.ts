import { ChatOpenAI } from '@langchain/openai';
import { SystemMessage, HumanMessage } from '@langchain/core/messages';
import { config } from '../../config/env';

export interface AgentResponse {
  content: string;
  confidence: number;
  reasoning: string;
}

export const generateAgentResponse = async (
  systemPrompt: string, 
  userQuery: string, 
  history: string[] = []
): Promise<AgentResponse> => {
  
  if (!config.OPENAI_API_KEY) {
      console.warn('OPENAI_API_KEY not found. Using Mock Agent.');
      return {
          content: `Based on your query "${userQuery}", here is a simulated answer. (System Prompt: ${systemPrompt.substring(0, 50)}...)`,
          confidence: 0.9,
          reasoning: 'Mock mode active.',
      };
  }
  
  try {
    const model = new ChatOpenAI({
      apiKey: config.OPENAI_API_KEY,
      modelName: 'gpt-4o',
      temperature: 0.7,
    });

    const response = await model.invoke([
      new SystemMessage(systemPrompt),
      ...history.map(msg => new HumanMessage(msg)),
      new HumanMessage(userQuery),
    ]);

    return {
      content: response.content as string,
      confidence: 0.9,
      reasoning: 'Generated by AI.',
    };
  } catch (error) {
      console.error('Agent LLM Error:', error);
      return {
          content: 'I am having trouble processing your request right now.',
          confidence: 0,
          reasoning: 'Error encountered.',
      };
  }
};
